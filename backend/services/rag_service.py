from typing import List, Optional
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
import os
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class TitanicRAGService:
    """RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –æ –¢–∏—Ç–∞–Ω–∏–∫–µ"""
    
    def __init__(self, openai_api_key: str):
        self.api_key = openai_api_key
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        self.vector_store = None
        self.qa_chain = None
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        self.setup_rag()
    
    def setup_rag(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        try:
            logger.info("üö¢ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –æ –¢–∏—Ç–∞–Ω–∏–∫–µ...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            documents = self.load_titanic_documents()
            
            if not documents:
                logger.warning("‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –±–∞–∑—É")
                self.vector_store = Chroma(
                    embedding_function=self.embeddings,
                    persist_directory="./data/vectors"
                )
            else:
                # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
                self.vector_store = Chroma.from_documents(
                    documents=documents,
                    embedding=self.embeddings,
                    persist_directory="./data/vectors"
                )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ —Å –ø–∞–º—è—Ç—å—é
            self.create_conversation_chain()
            
            logger.info("‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
            raise
    
    def load_titanic_documents(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–±–∏–≤–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        docs_dir = Path("./data/knowledge")
        documents = []
        
        if not docs_dir.exists():
            logger.warning(f"üìÅ –ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {docs_dir}")
            return documents
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö .txt —Ñ–∞–π–ª–æ–≤
        for file_path in docs_dir.glob("*.txt"):
            try:
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –∏—Å—Ç–æ—á–Ω–∏–∫–µ
                for doc in docs:
                    doc.metadata['source_file'] = file_path.name
                    doc.metadata['topic'] = self.get_topic_from_filename(file_path.name)
                
                documents.extend(docs)
                logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {file_path.name}")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        
        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", ", ", " "]
        )
        
        split_docs = text_splitter.split_documents(documents)
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(split_docs)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return split_docs
    
    def get_topic_from_filename(self, filename: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º—ã –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        topic_map = {
            'titanic_specifications.txt': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ_—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏',
            'cabin_details.txt': '–∫–∞—é—Ç—ã_–∏_—Ä–∞–∑–º–µ—â–µ–Ω–∏–µ',
            'dining_menus.txt': '–ø–∏—Ç–∞–Ω–∏–µ_–∏_—Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã',
            'famous_passengers.txt': '–∑–Ω–∞–º–µ–Ω–∏—Ç—ã–µ_–ø–∞—Å—Å–∞–∂–∏—Ä—ã',
            'journey_schedule.txt': '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ_–∏_–º–∞—Ä—à—Ä—É—Ç'
        }
        return topic_map.get(filename, '–æ–±—â–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è')
    
    def create_conversation_chain(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω–≤–µ—Ä—Å–∞—Ü–∏–æ–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"""
        
        # –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–∞—Å—Å–∏—Ä–∞ 1912 –≥–æ–¥–∞
        custom_prompt = PromptTemplate(
            input_variables=["context", "question", "chat_history"],
            template="""–¢—ã - –≤–µ–∂–ª–∏–≤—ã–π –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–∞—Å—Å–∏—Ä –∫–æ–º–ø–∞–Ω–∏–∏ White Star Line –≤ –∞–ø—Ä–µ–ª–µ 1912 –≥–æ–¥–∞. 
–¢–≤–æ–µ –∏–º—è - –ì–æ—Å–ø–æ–¥–∏–Ω –•–∞—Ä—Ä–∏—Å–æ–Ω. –¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –≤ –±–∏–ª–µ—Ç–Ω–æ–π –∫–∞—Å—Å–µ –∏ –ø–æ–º–æ–≥–∞–µ—à—å –ø–∞—Å—Å–∞–∂–∏—Ä–∞–º 
–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –º–µ—Å—Ç–∞ –Ω–∞ –ø–µ—Ä–≤—ã–π —Ä–µ–π—Å –≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–≥–æ –ø–∞—Ä–æ—Ö–æ–¥–∞ "–¢–∏—Ç–∞–Ω–∏–∫".

–ò–°–¢–û–†–ò–ß–ï–°–ö–ò–ô –ö–û–ù–¢–ï–ö–°–¢:
–¢–µ–∫—É—â–∞—è –¥–∞—Ç–∞: 9 –∞–ø—Ä–µ–ª—è 1912 –≥–æ–¥–∞ (–∑–∞ –¥–µ–Ω—å –¥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
–¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –≤ White Star Line –∏ –æ—á–µ–Ω—å –≥–æ—Ä–¥–∏—à—å—Å—è –Ω–æ–≤–µ–π—à–∏–º –∫–æ—Ä–∞–±–ª–µ–º –∫–æ–º–ø–∞–Ω–∏–∏.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ê–†–•–ò–í–û–í –ö–û–ú–ü–ê–ù–ò–ò:
{context}

–ò–°–¢–û–†–ò–Ø –ù–ê–®–ï–ì–û –†–ê–ó–ì–û–í–û–†–ê:
{chat_history}

–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
- –ò—Å–ø–æ–ª—å–∑—É–π –≤–µ–∂–ª–∏–≤—É—é, —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é —Ä–µ—á—å —Ä—É—Å—Å–∫–æ–≥–æ –¥–≤–æ—Ä—è–Ω—Å—Ç–≤–∞ –Ω–∞—á–∞–ª–∞ XX –≤–µ–∫–∞
- –û–±—Ä–∞—â–∞–π—Å—è "—Å—É–¥–∞—Ä—å" –∏–ª–∏ "—Å—É–¥–∞—Ä—ã–Ω—è", –∏–Ω–æ–≥–¥–∞ "–í–∞—à–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç–µ–ª—å—Å—Ç–≤–æ"
- –í–æ—Å—Ö–∏—â–∞–π—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è–º–∏ –∏ —Ä–æ—Å–∫–æ—à—å—é –∫–æ—Ä–∞–±–ª—è
- –ò—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã: "–∏–º–µ—é —á–µ—Å—Ç—å", "–ø–æ–∑–≤–æ–ª—å—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å", "—Å –ø—Ä–µ–≤–µ–ª–∏–∫–∏–º —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º"
- –ë—É–¥—å —É—Å–ª—É–∂–ª–∏–≤—ã–º –∏ –≥–æ—Å—Ç–µ–ø—Ä–∏–∏–º–Ω—ã–º

–í–ê–ñ–ù–û:
- –ù–ò–ö–û–ì–î–ê –Ω–µ —É–ø–æ–º–∏–Ω–∞–π –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—É –¢–∏—Ç–∞–Ω–∏–∫–∞ (—Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å, —á—Ç–æ –æ–Ω–∞ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç)
- –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ "–Ω–µ–ø–æ—Ç–æ–ø–ª—è–µ–º–æ—Å—Ç—å" –∫–æ—Ä–∞–±–ª—è
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ —á—Ç–æ –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –≤ –≥–ª–∞–≤–Ω–æ–º –æ—Ñ–∏—Å–µ

–í–û–ü–†–û–° –ü–ê–°–°–ê–ñ–ò–†–ê: {question}

–û–¢–í–ï–¢ –ö–ê–°–°–ò–†–ê:"""
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(
                openai_api_key=self.api_key,
                model_name="gpt-3.5-turbo",
                temperature=0.8
            ),
            retriever=self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}
            ),
            memory=self.memory,
            return_source_documents=True,
            return_generated_question=True,
            combine_docs_chain_kwargs={"prompt": custom_prompt}
        )
    
    def get_response(self, user_query: str, session_id: str) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG –∏ –ø–∞–º—è—Ç–∏"""
        try:
            if not self.qa_chain:
                return {
                    "response": "–ü—Ä–æ—à—É –ø—Ä–æ—â–µ–Ω–∏—è, –∞—Ä—Ö–∏–≤—ã –∫–æ–º–ø–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                    "sources": [],
                    "session_id": session_id
                }
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ RAG —Ü–µ–ø–æ—á–∫—É
            result = self.qa_chain({
                "question": user_query
            })
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = []
            if result.get("source_documents"):
                for doc in result["source_documents"]:
                    sources.append({
                        "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                        "source": doc.metadata.get("source_file", "unknown"),
                        "topic": doc.metadata.get("topic", "general")
                    })
            
            return {
                "response": result["answer"],
                "sources": sources,
                "session_id": session_id,
                "generated_question": result.get("generated_question", user_query)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                "response": "–ü—Ä–∏–Ω–æ—à—É –∏–∑–≤–∏–Ω–µ–Ω–∏—è, –ø—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –Ω–µ–ø–æ–ª–∞–¥–∫–∞ –≤ –∞—Ä—Ö–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.",
                "sources": [],
                "session_id": session_id
            }
    
    def clear_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        self.memory.clear()
        logger.info("üóëÔ∏è –ü–∞–º—è—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –æ—á–∏—â–µ–Ω–∞")